#!/usr/bin/env python

from __future__ import print_function

"""
qtip

A "tandem simulator," which wraps an alignment tool as it runs, eavesdrops on
input and output, simulates a new dataset similar to the input data, aligns
it, uses those alignments as training data to build a model to predict MAPQ,
then re-calcualtes MAPQs for the original input using that predictor.
"""

import os
from os.path import join, getsize
from subprocess import Popen, PIPE
import abc
import sys
import time
import logging
import resource
import datetime
import glob

__author__ = "Ben Langmead"
__email__ = "langmea@cs.jhu.edu"

bin_dir = os.path.dirname(os.path.realpath(__file__))


class Timing(object):

    def __init__(self):
        self.labs = []
        self.timers = dict()

    def start_timer(self, lab):
        self.labs.append(lab)
        self.timers[lab] = time.time()

    def end_timer(self, lab):
        self.timers[lab] = time.time() - self.timers[lab]

    def __str__(self):
        ret = []
        for lab in self.labs:
            ret.append('\t'.join([lab, str(self.timers[lab])]))
        return '\n'.join(ret) + '\n'


def sanity_check_binary(exe):
    if not os.path.exists(exe):
        raise RuntimeError('Binary "%s" was not built; see Building Qtip '
                           'section of README.md for instructions on building '
                           'the Qtip binaries' % exe)
    else:
        if not os.access(exe, os.X_OK):
            raise RuntimeError('Binary "%s" exists but is not executable' % exe)


def mkdir_quiet(dr):
    # Create output directory if needed
    import errno
    if not os.path.isdir(dr):
        try:
            os.makedirs(dr)
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise


def recursive_size(dr):
    tot = 0
    for root, dirs, files in os.walk(dr):
        tot += sum(getsize(join(root, name)) for name in files)
    return tot


def seed_all(seed):
    import numpy as np
    import random
    random.seed(seed)
    np.random.seed(seed)


def _nop():
    pass


def _at_least_one_read_aligned(sam_fn):
    with open(sam_fn) as fh:
        for ln in fh:
            if ln[0] != '@':
                return True
    return False


def _cat(fns, dst_fn):
    import shutil
    with open(dst_fn, 'wb') as ofh:
        for fn in fns:
            with open(fn, 'rb') as fh:
                shutil.copyfileobj(fh, ofh)


def _get_peak_gb():
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1024.0 * 1024.0)


def go(args, aligner_args, aligner_unpaired_args, aligner_paired_args):

    print('Qtip ' + open(join(bin_dir, 'VERSION')).read().rstrip(), file=sys.stderr)
    tim = Timing()
    tim.start_timer('Overall')

    # Set up logger
    format_str = '%(asctime)s:%(levelname)s:%(message)s'
    level = logging.DEBUG if args['verbose'] else logging.INFO
    logging.basicConfig(format=format_str, datefmt='%m/%d/%y-%H:%M:%S', level=level)

    vanilla = args['vanilla_output'] is not None
    if vanilla and args['output_directory'] is not None:
        logging.warning("--vanilla-output overrides and disables --output-directory")
        args['output_directory'] = None

    if not vanilla and args['output_directory'] is None:
        args['output_directory'] = 'out'

    if vanilla and args['keep_intermediates']:
        logging.warning("--vanilla-output overrides and disables --keep-intermediates")
        args['keep_intermediates'] = False

    # Create output directory if needed
    odir = None
    if args['output_directory'] is not None:
        odir = args['output_directory']
        mkdir_quiet(odir)

    # Copy logging messages to file
    if args['output_directory'] is not None:
        fn = join(odir, 'ts_logs' + datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S') + '.txt')
        fh = logging.FileHandler(fn)
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(logging.Formatter(format_str))
        logging.getLogger('').addHandler(fh)

    # Set up memory profilier
    hp = None
    if args['profile_memory']:
        from guppy import hpy
        hp = hpy()

    if args['U'] is not None and args['m1'] is not None:
        raise RuntimeError('Input must consist of only unpaired or only paired-end reads')

    # Start building alignment command; right now we support Bowtie 2, BWA-MEM and SNAP
    from bowtie2 import Bowtie2
    from bwamem import BwaMem
    from snap import SnapAligner
    aligner_class = Bowtie2
    align_cmd = None
    if args['aligner'] == 'bowtie2':
        align_cmd = 'bowtie2 '
        if args['bt2_exe'] is not None:
            align_cmd = args['bt2_exe'] + " "
        aligner_args.extend(['--mapq-extra'])
    elif args['aligner'] == 'bwa-mem':
        align_cmd = 'bwa mem '
        if args['bwa_exe'] is not None:
            align_cmd = args['bwa_exe'] + ' mem '
        aligner_class = BwaMem
    elif args['aligner'] == 'snap':
        align_cmd = 'snap-aligner '
        if args['snap_exe'] is not None:
            align_cmd = args['snap_exe'] + ' '
        aligner_class = SnapAligner
        aligner_args.extend(['-='])
    elif args['aligner'] is not None:
        raise RuntimeError('Aligner not supported: "%s"' % args['aligner'])

    # for storing temp files and keep track of how big they get
    from tempman import TemporaryFileManager
    temp_man = TemporaryFileManager(args['temp_directory'])

    def _get_trial_subdir(_trial_multi, _triali):
        return join(odir, 'trial%d' % _triali) if _trial_multi else odir

    def _get_pass1_file_prefixes(_trial_multi, _triali):
        """
        Return the file prefix that should be used for naming (a) input record
        and (b) tandem model/read files generated by qtip-parse when parsing
        input SAM.
        """
        if args['keep_intermediates']:
            return join(odir, 'input_intermediates'), \
                   join(_get_trial_subdir(_trial_multi, _triali), 'tandem_intermediates'), \
                   _nop
        else:
            dr_inp = temp_man.get_dir('input_intermediates')
            dr_tan = temp_man.get_dir('tandem_intermediates')
            assert os.path.isdir(dr_inp)
            assert os.path.isdir(dr_tan)

            def _purge():
                temp_man.remove_group('input_intermediates')
                # don't purse tandem intermediates yet
            return join(dr_inp, 'tmp'), join(dr_tan, 'tmpinp'), _purge

    def _get_pass2_file_prefix(_trial_multi, _triali):
        """
        Return the file prefix that should be used for naming intermediate
        files generated by qtip-parse when parsing the tandem SAM.
        """
        if args['keep_intermediates']:
            return join(_get_trial_subdir(_trial_multi, _triali), 'tandem_intermediates'), _nop
        else:
            dr = temp_man.get_dir('tandem_intermediates')
            assert os.path.isdir(dr)

            def _purge():
                temp_man.remove_group('tandem_intermediates')
            return join(dr, 'tmptan'), _purge

    parse_input_exe = "%s/qtip-parse" % bin_dir
    rewrite_exe = "%s/qtip-rewrite" % bin_dir

    def _get_input_sam_fn():
        """ input.sam goes in the toplevel output directory """
        if args['keep_intermediates']:
            return join(odir, 'input.sam'), _nop
        else:
            dr = temp_man.get_dir('input_alignments')

            def _purge():
                temp_man.remove_group('input_alignments')
            return join(dr, 'tmp'), _purge

    def _compose(_triali=None, subsamp=None, incmapq=None, test=None, join_with=None):
        subdirs = []
        if join_with is None:
            subdirs = [odir]
        if _triali is not None:
            subdirs.append('trial%d' % _triali)
        if subsamp is not None:
            subdirs.append(subsamp)
        if incmapq is not None:
            subdirs.append('mapq_excluded' if incmapq else 'mapq_included')
        if test is not None:
            subdirs.append('test' if test else 'train')
        if join_with is None:
            return join(*subdirs)
        else:
            return join_with.join(subdirs)

    class FileDispenser(object):
        __metaclass__ = abc.ABCMeta

        def __init__(self, _temp_man):
            self.to_remove = []
            self.purged = False
            self.temp_man = _temp_man

        def __del__(self):
            self.purge()

        @abc.abstractmethod
        def get(self, _triali=None, subsamp=None, incmapq=None, test=None):
            pass

        def purge(self):
            if not self.purged:
                for tr in self.to_remove:
                    for fn in glob.glob(tr):
                        os.remove(fn)
                self.purged = True

    class GetPredictionFile(FileDispenser):

        def __init__(self, _temp_man):
            super(GetPredictionFile, self).__init__(_temp_man)
            self.temp_dir = None
            self.last_prefix = None

        def get(self, _triali=None, subsamp=None, incmapq=None, test=None):
            if args['keep_intermediates']:
                od = _compose(_triali, subsamp, incmapq, test)
                mkdir_quiet(od)
                ret_pred = join(od, 'predictions')
                ret_assess = join(od, 'predictions_assess')
            else:
                assert self.temp_man is not None
                if self.temp_dir is None:
                    self.temp_dir = self.temp_man.get_dir('prediction_files')
                pref = _compose(_triali, subsamp, incmapq, test, join_with='_')
                ret_pred = join(self.temp_dir, '_'.join([pref, 'predictions']))
                ret_assess = join(self.temp_dir, '_'.join([pref, 'predictions_assess']))
            if test is None or test:
                self.last_prefix = ret_pred
            return ret_pred, ret_assess

        def purge(self):
            super(GetPredictionFile, self).purge()
            if self.temp_dir is not None:
                self.temp_man.remove_group('prediction_files')

    class GetFinalSamFile(FileDispenser):

        def __init__(self, _temp_man):
            super(GetFinalSamFile, self).__init__(_temp_man)

        def get(self, _triali=None, subsamp=None, incmapq=None, test=None):
            if vanilla:
                return args['vanilla_output']
            else:
                return join(_compose(_triali, subsamp, incmapq, None), 'final.sam')

    class GetTandemSamFile(FileDispenser):

        def __init__(self, temp_man):
            super(GetTandemSamFile, self).__init__(temp_man)
            self.temp_dir = None

        def get(self, _triali=None, subsamp=None, incmapq=None, test=None):
            if args['keep_intermediates']:
                od = _compose(_triali, None, None, None)
                mkdir_quiet(od)
                ret_u_tandsam = join(od, 'tandem_unp.sam')
                ret_p_tandsam = join(od, 'tandem_paired.sam')
                ret_b_tandsam = join(od, 'tandem_both.sam')
            else:
                assert self.temp_man is not None
                if self.temp_dir is None:
                    self.temp_dir = self.temp_man.get_dir('tandem_alignments')
                pref = _compose(_triali, None, None, None, join_with='_')
                ret_u_tandsam = join(self.temp_dir, '_'.join([pref, 'tandem_unp.sam']))
                ret_p_tandsam = join(self.temp_dir, '_'.join([pref, 'tandem_paired.sam']))
                ret_b_tandsam = join(self.temp_dir, '_'.join([pref, 'tandem_both.sam']))
            return ret_u_tandsam, ret_p_tandsam, ret_b_tandsam

        def purge(self):
            super(GetTandemSamFile, self).purge()
            self.temp_man.remove_group('tandem_alignments')

    tandemsam_file_getter = GetTandemSamFile(temp_man)
    pred_file_getter = GetPredictionFile(temp_man)
    finalsam_file_getter = GetFinalSamFile(temp_man)

    def _get_passthrough_args(exe):
        op = Popen(exe, stdout=PIPE).communicate()[0]
        ls = []
        for ar in op.strip().split(b' '):
            ar_underscore = ar.replace(b'-', b'_')
            if ar_underscore in args:
                logging.debug('  passing through argument "%s"="%s"' % (ar, str(args[ar_underscore])))
                ls.append(ar)
                ls.append(str(args[ar_underscore]))
        return ' '.join(ls)

    def _wait_for_aligner(_al):
        ret = _al.pipe.poll()
        while ret is None:
            time.sleep(0.5)
            ret = _al.pipe.poll()
        return ret

    def _exists_and_nonempty(_fn):
        return os.path.exists(_fn) and os.stat(_fn).st_size > 0

    def _have_unpaired_tandem_reads(prefix):
        ufn = prefix + '_reads_u.fastq'
        return _exists_and_nonempty(ufn)

    def _have_paired_tandem_reads(prefix):
        cfn = prefix + '_reads_c_1.fastq'
        dfn = prefix + '_reads_d_1.fastq'
        bfn = prefix + '_reads_b_1.fastq'
        return _exists_and_nonempty(cfn) or _exists_and_nonempty(dfn) or _exists_and_nonempty(bfn)

    def _unpaired_tandem_reads(prefix):
        ufn = prefix + '_reads_u.fastq'
        return [ufn] if _exists_and_nonempty(ufn) else []

    def _paired_tandem_reads(prefix, single_file=False):
        from operator import itemgetter
        ls = []
        cfn1, cfn2 = prefix + '_reads_c_1.fastq', prefix + '_reads_c_2.fastq'
        if _exists_and_nonempty(cfn1):
            ls.append((cfn1, cfn2))
        dfn1, dfn2 = prefix + '_reads_d_1.fastq', prefix + '_reads_d_2.fastq'
        if _exists_and_nonempty(dfn1):
            ls.append((dfn1, dfn2))
        bfn1, bfn2 = prefix + '_reads_b_1.fastq', prefix + '_reads_b_2.fastq'
        if _exists_and_nonempty(bfn1):
            ls.append((bfn1, bfn2))
        if len(ls) > 1 and single_file:
            fn1, fn2 = prefix + '_reads_combined_1.fastq', prefix + '_reads_combined_2.fastq'
            _cat(map(itemgetter(0), ls), fn1)
            _cat(map(itemgetter(1), ls), fn2)
            ls = [(fn1, fn2)]
        return ls

    # ##################################################
    # 1. Align input reads
    # ##################################################

    input_sam_fn, input_sam_purge = _get_input_sam_fn()

    def _do_align_reads():
        tim.start_timer('Aligning input reads')
        logging.info('Command for aligning input data: "%s"' % align_cmd)
        aligner = aligner_class(
            align_cmd,
            aligner_args,
            aligner_unpaired_args,
            aligner_paired_args,
            args['index'],
            unpaired=args['U'],
            paired=None if args['m1'] is None else zip(args['m1'], args['m2']),
            sam=input_sam_fn)

        logging.debug('  waiting for aligner to finish...')
        if _wait_for_aligner(aligner) != 0:
            logging.error("Non-zero exitlevel from aligner")
            raise RuntimeError('Non-zero exitlevel from aligner')
        logging.debug('  aligner finished; results in "%s"' % input_sam_fn)
        tim.end_timer('Aligning input reads')

        if not _at_least_one_read_aligned(input_sam_fn):
            logging.warning("None of the input reads aligned; exiting")
            sys.exit(0)

        if args['profile_memory']:
            print(hp.heap(), file=sys.stderr)

    def _do_align_reads_is_done():
        return os.path.exists(input_sam_fn)

    if not vanilla and _do_align_reads_is_done():
        logging.info('Skipping alignment because "%s" already exists' % input_sam_fn)
    else:
        _do_align_reads()

    ntrials = args['trials']
    trial_multi = ntrials > 1
    orig_seed = args['seed']
    for triali in range(ntrials):

        # re-seed pseudo-random generator
        args['seed'] = (abs(hash((orig_seed, triali, 0))) % 2147483562)+1
        seed_all(args['seed'])

        if args['keep_intermediates']:
            mkdir_quiet(_get_trial_subdir(trial_multi, triali))
        skipped_all = True

        # ##################################################
        # 2. Parse input SAM
        # ##################################################

        pass1_prefix_inp, pass1_prefix_tan, pass1_cleanup = _get_pass1_file_prefixes(trial_multi, triali)

        def _do_parse_input_sam():
            tim.start_timer('Parsing input alignments')
            sanity_check_binary(parse_input_exe)
            input_parse_cmd = "%s ifs -- %s -- %s -- %s -- %s -- %s" % \
                (parse_input_exe, _get_passthrough_args(parse_input_exe), input_sam_fn, ' '.join(args['ref']),
                 pass1_prefix_inp, pass1_prefix_tan)
            logging.info('  running "%s"' % input_parse_cmd)
            ret = os.system(input_parse_cmd)
            if ret != 0:
                raise RuntimeError("qtip-parse returned %d" % ret)
            logging.debug('  parsing finished; results in "%s*" and "%s*"' %
                          (pass1_prefix_inp, pass1_prefix_tan))
            tim.end_timer('Parsing input alignments')

            if args['profile_memory']:
                print(hp.heap(), file=sys.stderr)

        def _do_parse_input_sam_is_done():
            exts = ['_rec_u.',
                    '_rec_b.',
                    '_rec_c.',
                    '_rec_d.']
            for ex in exts:
                if not os.path.exists(pass1_prefix_inp + ex + 'npy'):
                    return False
                if not os.path.exists(pass1_prefix_inp + ex + 'meta'):
                    return False
            exts = ['_reads_u.fastq',
                    '_reads_b_1.fastq',
                    '_reads_c_1.fastq',
                    '_reads_d_1.fastq',
                    '_reads_b_2.fastq',
                    '_reads_c_2.fastq',
                    '_reads_d_2.fastq']
            for ex in exts:
                if not os.path.exists(pass1_prefix_tan + ex):
                    return False
            return True

        if not vanilla and _do_parse_input_sam_is_done():
            logging.info('Skipping parsing input sam because outputs at "%s*" and "%s*" already exist' %
                         (pass1_prefix_inp, pass1_prefix_tan))
        else:
            _do_parse_input_sam()
            skipped_all = False

        # ##################################################
        # 3. Align tandem reads
        # ##################################################

        tandem_sam_u_fn, tandem_sam_p_fn, tandem_sam_b_fn =\
            tandemsam_file_getter.get(triali if trial_multi else None)
        tandem_sams = [tandem_sam_u_fn, tandem_sam_p_fn, tandem_sam_b_fn]

        def _do_align_tandem_reads():
            tim.start_timer('Aligning tandem reads')
            assert _have_unpaired_tandem_reads(pass1_prefix_tan) or _have_paired_tandem_reads(pass1_prefix_tan)
            if _have_unpaired_tandem_reads(pass1_prefix_tan) and \
                    _have_paired_tandem_reads(pass1_prefix_tan) and \
                    aligner_class.supports_mix():
                logging.info('Aligning tandem reads (mix)')
                aligner = aligner_class(
                    align_cmd,
                    aligner_args,
                    aligner_unpaired_args,
                    aligner_paired_args,
                    args['index'],
                    unpaired=_unpaired_tandem_reads(pass1_prefix_tan),
                    paired=_paired_tandem_reads(pass1_prefix_tan, single_file=True),
                    sam=tandem_sam_b_fn,
                    input_format='fastq')
                _wait_for_aligner(aligner)
                logging.debug('Finished aligning unpaired and paired-end tandem reads')
            else:
                if _have_unpaired_tandem_reads(pass1_prefix_tan):
                    logging.info('Aligning tandem reads (unpaired)')
                    aligner = aligner_class(
                        align_cmd,
                        aligner_args,
                        aligner_unpaired_args,
                        aligner_paired_args,
                        args['index'],
                        unpaired=_unpaired_tandem_reads(pass1_prefix_tan),
                        sam=tandem_sam_u_fn,
                        input_format='fastq')
                    _wait_for_aligner(aligner)
                    logging.debug('Finished aligning unpaired tandem reads')
                if _have_paired_tandem_reads(pass1_prefix_tan):
                    logging.info('Aligning tandem reads (paired)')
                    aligner = aligner_class(
                        align_cmd,
                        aligner_args,
                        aligner_unpaired_args,
                        aligner_paired_args,
                        args['index'],
                        paired=_paired_tandem_reads(pass1_prefix_tan, single_file=True),
                        sam=tandem_sam_p_fn,
                        input_format='fastq')
                    _wait_for_aligner(aligner)
                    logging.debug('Finished aligning paired tandem reads')
            if len(list(filter(_exists_and_nonempty, tandem_sams))) == 0:
                raise RuntimeError('No tandem reads written')
            tim.end_timer('Aligning tandem reads')

            if args['profile_memory']:
                print(hp.heap(), file=sys.stderr)

        def _do_align_tandem_reads_is_done():
            return len(list(filter(_exists_and_nonempty, tandem_sams))) > 0

        if not vanilla and _do_align_tandem_reads_is_done():
            assert skipped_all  # doesn't make sense to run one step then skip a later step
            logging.info('Skipping tandem read alignment since output files exist (%s)' % str(tandem_sams))
        else:
            _do_align_tandem_reads()
            skipped_all = False

        # ##################################################
        # 4. Parse tandem alignments
        # ##################################################

        pass2_prefix, pass2_cleanup = _get_pass2_file_prefix(trial_multi, triali)

        def _do_parse_tandem_alignments():
            tim.start_timer('Parsing tandem alignments')
            sanity_check_binary(parse_input_exe)
            parse_cmd = "%s f -- %s -- %s -- %s -- %s" % \
                        (parse_input_exe, _get_passthrough_args(parse_input_exe),
                         ' '.join(filter(_exists_and_nonempty, tandem_sams)), ' '.join(args['ref']), pass2_prefix)
            logging.info('  running "%s"' % parse_cmd)
            ret = os.system(parse_cmd)
            if ret != 0:
                raise RuntimeError("qtip-parse returned %d" % ret)
            logging.debug('  parsing finished; results in "%s.*"' % pass2_prefix)
            tandemsam_file_getter.purge()  # delete tandem-alignment intermediates
            tim.end_timer('Parsing tandem alignments')

            if args['profile_memory']:
                print(hp.heap(), file=sys.stderr)

        def _do_parse_tandem_alignments_is_done():
            exts = ['_rec_u.',
                    '_rec_b.',
                    '_rec_c.',
                    '_rec_d.']
            for ex in exts:
                if not os.path.exists(pass2_prefix + ex + 'npy'):
                    return False
                if not os.path.exists(pass2_prefix + ex + 'meta'):
                    return False
            return True

        if not vanilla and _do_parse_tandem_alignments_is_done():
            assert skipped_all  # doesn't make sense to run one step then skip a later step
            logging.info('Skipping parsing tandem sam because outputs at prefix "%s" already exist' % pass2_prefix)
        else:
            skipped_all = False
            _do_parse_tandem_alignments()

        # ##################################################
        # 5. Predict
        # ##################################################

        triali_or_none = triali if trial_multi else None

        def _do_predictions():
            tim.start_timer('Make MAPQ predictions')
            logging.info('Making MAPQ predictions')
            logging.info('  instantiating feature table readers')
            from feature_table import FeatureTableReader
            tab_ts = FeatureTableReader(pass1_prefix_inp, chunksize=args['max_rows'])
            tab_tr = FeatureTableReader(pass2_prefix, chunksize=args['max_rows'])

            def _do_predict(fit, sampdir, include_mapq, test_or_none):
                test = test_or_none is None or test_or_none
                pred_prefix, assess_prefix = pred_file_getter.get(triali_or_none, sampdir, include_mapq, test_or_none)
                tab = tab_ts if test else tab_tr
                pred = fit.predict(tab, pred_prefix, assess_prefix,
                                   dedup=args['collapse'], training=not test,
                                   calc_summaries=args['assess_accuracy'],
                                   prediction_mem_limit=args['assess_limit'],
                                   heap_profiler=hp, include_mapq=include_mapq,
                                   multiprocess=False, n_multi=1)
                if not vanilla and pred.can_assess():
                    logging.info('  writing accuracy measures')
                    od = _compose(triali_or_none, sampdir, include_mapq, test_or_none)
                    mkdir_quiet(od)
                    pred.write_rocs(join(od, 'roc'))
                    pred.write_top_incorrect(join(od, 'top_incorrect.csv'))
                    pred.write_summary_measures(join(od, 'summary.csv'))
                if args['profile_memory']:
                    print(hp.heap(), file=sys.stderr)
                return pred

            def _do_fit(fraction, sampdir, fam, include_mapq):
                from fit import MapqFit
                fit = MapqFit(tab_tr, fam,
                              sample_fraction=fraction, heap_profiler=hp,
                              include_mapq=include_mapq,
                              reweight_ratio=args['reweight_ratio'],
                              reweight_mapq=args['reweight_mapq'],
                              reweight_mapq_offset=args['reweight_mapq_offset'],
                              no_oob=args['no_oob'])
                if not vanilla:
                    logging.info('  writing feature importances')
                    od = _compose(triali_or_none, sampdir, include_mapq, None)
                    mkdir_quiet(od)
                    fit.write_feature_importances(join(od, 'featimport'))
                    fit.write_parameters(join(od, 'params'))
                logging.info('    finishing _do_fit (peak=%0.2fGB)' % _get_peak_gb())
                if args['profile_memory']:
                    print(hp.heap(), file=sys.stderr)
                return fit

            def _fits_and_predictions(fraction, sampdir, fam, include_mapq):
                logging.info('  fitting to tandem alignments')
                fit = _do_fit(fraction, sampdir, fam, include_mapq)
                logging.info('Making predictions for input alignments (peak=%0.2fGB)' % _get_peak_gb())
                _do_predict(fit, sampdir, include_mapq, True if args['predict_for_training'] else None)
                if args['predict_for_training']:
                    logging.info('Making predictions for tandem (training) alignments')
                    _do_predict(fit, sampdir, include_mapq, False)

            def _all_fits_and_predictions():
                from model_fam import model_family
                fractions = list(map(float, args['subsampling_series'].split(',')))
                sampdir = None
                for fraction in fractions:
                    if fraction < 0.0 or fraction > 1.0:
                        raise RuntimeError('Bad subsampling fraction: %f' % fraction)
                    if len(fractions) > 1:
                        sampdir = 'sample' + str(fraction)
                        logging.info('  trying sampling fraction %0.02f%%' % (100.0 * fraction))
                    nt = 1
                    for i in range(nt):
                        if nt > 1:
                            logging.info('  trial %d' % (i+1))
                        seed = (abs(hash((orig_seed, triali, i+1))) % 2147483562)+1
                        seed_all(seed)
                        logging.info('  pseudo-random seed %d' % seed)
                        fam = model_family(args, seed)
                        _fits_and_predictions(fraction, sampdir, fam, False if args['try_include_mapq'] else None)
                        if args['try_include_mapq']:
                            _fits_and_predictions(fraction, sampdir, fam, True)

            # done with the input intermediates
            _all_fits_and_predictions()
            pass1_cleanup()
            pass2_cleanup()
            tim.end_timer('Make MAPQ predictions')

            if args['profile_memory']:
                print(hp.heap(), file=sys.stderr)

        mult_trials = trial_multi
        mult_subsamps = args['subsampling_series'].count(',') > 0
        mult_mapq = args['try_include_mapq']
        mult_test = args['predict_for_training']
        mult = mult_trials or mult_subsamps or mult_mapq or mult_test

        def _do_predictions_is_done():
            if mult or not args['keep_intermediates']:
                # if mult, it's too hard to determine if *all* predictions are done
                return False
            else:
                prefix, _ = pred_file_getter.get()
                return len(glob.glob(prefix + ".*.npy")) > 0

        if not vanilla and _do_predictions_is_done():
            assert skipped_all  # doesn't make sense to run one step then skip a later step
            # Actually do the prefix check
            logging.info('Skipping prediction because at least one prediction file exists')
        else:
            _do_predictions()
            skipped_all = False

        # ##################################################
        # 6. Rewrite SAM
        # ##################################################

        out_sz = None
        if not args['skip_rewrite']:
            final_sam = finalsam_file_getter.get(triali_or_none)

            def _do_rewrite():
                tim.start_timer('Rewrite SAM file')
                sanity_check_binary(rewrite_exe)
                cmd = "%s %s -- %s -- %s -- %s" % \
                      (rewrite_exe, _get_passthrough_args(rewrite_exe), input_sam_fn,
                       ' '.join(glob.glob(pred_file_getter.last_prefix + '.*.npy')), final_sam)
                logging.info('  running "%s"' % cmd)
                ret = os.system(cmd)
                if ret != 0:
                    raise RuntimeError("qtip-rewrite returned %d" % ret)
                logging.debug('  rewriting finished; results in %s' % final_sam)
                input_sam_purge()
                pred_file_getter.purge()  # from this trial
                tim.end_timer('Rewrite SAM file')

                if args['profile_memory']:
                    print(hp.heap(), file=sys.stderr)

            def _do_rewrite_is_done():
                return _exists_and_nonempty(final_sam)

            if not vanilla and _do_rewrite_is_done():
                assert skipped_all  # doesn't make sense to run one step then skip a later step
                logging.info('Skipping rewriting because "%s" exists' % final_sam)
            else:
                skipped_all = False
                _do_rewrite()

            out_sz = getsize(final_sam)
            logging.info('Output SAM size: %0.2fMB' % (out_sz / (1024.0 * 1024)))

        if skipped_all:
            logging.warning('Skipped every step!  All outputs exist in output directory "%s"' %
                            _get_trial_subdir(trial_multi, triali))
            return

        logging.info('Purging temporaries')
        temp_man.purge()

        def _pct_output_sam(amt):
            if out_sz is not None:
                return ' (%0.2f%% of output SAM)' % (100.0 * amt / out_sz)
            else:
                return ''

        peak_tmp = temp_man.peak_size
        logging.info('Peak temporary file size: %0.2fMB%s' % (peak_tmp / (1024.0 * 1024),
                                                              _pct_output_sam(peak_tmp)))
        if not vanilla:
            tot_sz = recursive_size(odir)
            logging.info('Total size of output directory: %0.2fMB%s' % (tot_sz / (1024.0 * 1024),
                                                                        _pct_output_sam(tot_sz)))

    self_peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    child_peak = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss
    logging.info('Peak memory usage (RSS) of Python wrapper: %0.5fGB' % (self_peak / (1024.0 * 1024.0)))
    logging.info('Peak memory usage (RSS) of children: %0.5fGB' % (child_peak / (1024.0 * 1024.0)))
    logging.info('Memory overhead: %0.3f%%' % (0 if child_peak == 0 else 100.0 * self_peak/child_peak))

    tim.end_timer('Overall')
    for ln in str(tim).split('\n'):
        if len(ln) > 0:
            logging.info(ln)
    if not vanilla:
        with open(join(odir, 'timing.tsv'), 'w') as fh:
            fh.write(str(tim))

    if 'Aligning input reads' in tim.timers:
        logging.info('Time overhead: %0.01f%%' % (100.0 * (tim.timers['Overall'] - tim.timers['Aligning input reads']) /
                                                  tim.timers['Aligning input reads']))


def add_args(parser):

    # Overall arguments
    parser.add_argument('--ref', metavar='path', type=str, nargs='+',
                        required=True,
                        help='FASTA file, or many FASTAs separated by spaces, '
                             'containing reference genome sequences')
    parser.add_argument('--U', metavar='path', type=str, nargs='+',
                        help='Unpaired read FASTQ file name, or many FASTQ '
                             'file names separated by spaces')
    parser.add_argument('--m1', metavar='path', type=str, nargs='+',
                        help='Mate 1 FASTQ file name, or many FASTQ file names '
                             'separated by spaces; must be specified in same '
                             'order as --m2')
    parser.add_argument('--m2', metavar='path', type=str, nargs='+',
                        help='Mate 2 FASTQ file name, or many FASTQ file names '
                             'separated by spaces; must be specified in same '
                             'order as --m1')
    parser.add_argument('--index', metavar='path', type=str,
                        help='Index file to use; specify the appropriate '
                             'prefix, e.g. Bowtie 2 index file name without '
                             'the .X.bt2 suffix.')
    parser.add_argument('--seed', metavar='int', type=int, default=99099,
                        required=False,
                        help='Integer to initialize pseudo-random generator')

    # Qtip-parse: input model
    parser.add_argument('--max-allowed-fraglen', metavar='int', type=int,
                        default=100000, required=False,
                        help='When simulating fragments, longer fragments are '
                             'truncated to this length')
    parser.add_argument('--input-model-size', metavar='int', type=int,
                        default=30000, required=False,
                        help='Maximum # templates to keep when building an '
                             'input model. There are 4 separate models for '
                             'each alignment category and this governs the '
                             'maximum for all 4.')

    # Qtip-parse: simulator
    parser.add_argument('--sim-unp-min', metavar='int', type=int,
                        default=30000, required=False,
                        help='If predictions for unpaired reads '
                             'are needed, simulate at least this # of '
                             'unpaired reads.')
    parser.add_argument('--sim-conc-min', metavar='int', type=int,
                        default=30000, required=False,
                        help='If predictions for concordantly aligned reads '
                             'are needed, simulate at least this # of '
                             'concordant pairs.')
    parser.add_argument('--sim-disc-min', metavar='int', type=int,
                        default=10000, required=False,
                        help='If predictions for discordantly aligned reads '
                             'are needed, simulate at least this # of '
                             'discordant pairs.')
    parser.add_argument('--sim-bad-end-min', metavar='int', type=int,
                        default=10000, required=False,
                        help='If predictions for ends with an unaligned mate '
                             'are needed, simulate at least this # of pairs '
                             'with a bad end.')
    parser.add_argument('--sim-function', metavar='sqrt|linear|const', type=str,
                        required=False, default='sqrt',
                        help='Function giving # of tandem reads to simulate '
                             'in a category; parameter is the number of '
                             'input reads.  See also: --sim-factor.')
    parser.add_argument('--sim-factor', metavar='factor', type=float,
                        default=45.0, required=False,
                        help='This is multiplied with X (if '
                             '--sim-function=linear) or with sqrt(X) '
                             '(if --sim-function=sqrt) or with 1 (if '
                             '--sim-function=const) to calculate # '
                             'tandem reads to simulate in a given category, '
                             'where X is # of input reads in that category.')

    # Qtip-parse: correctness
    parser.add_argument('--wiggle', metavar='int', type=int, default=30,
                        required=False,
                        help='Wiggle room to allow in starting position when '
                             'determining whether alignment is correct')

    # Aligner
    parser.add_argument('--bt2-exe', metavar='path', type=str,
                        help='Path to Bowtie 2 aligner exe, "bowtie2"')
    parser.add_argument('--bwa-exe', metavar='path', type=str,
                        help='Path to BWA-MEM aligner exe, "bwa"')
    parser.add_argument('--snap-exe', metavar='path', type=str,
                        help='Path to SNAP aligner exe, "snap-aligner"')
    parser.add_argument('--aligner', metavar='name', default='bowtie2',
                        type=str,
                        help='Which aligner to use: bowtie2 | bwa-mem | snap')

    # SAM rewriting
    parser.add_argument('--write-orig-mapq', action='store_const',
                        const=True, default=False,
                        help='Write original MAPQ as an extra field in '
                             'output SAM')
    parser.add_argument('--write-precise-mapq', action='store_const',
                        const=True, default=False,
                        help='Write a more precise MAPQ prediction as an '
                             'extra field in output SAM')
    parser.add_argument('--orig-mapq-flag', metavar='XX:X', type=str,
                        default="Zm:Z", required=False,
                        help='If --write-orig-mapq is specified, store '
                             'original MAPQ in this extra SAM field')
    parser.add_argument('--precise-mapq-flag', metavar='XX:X', type=str,
                        default="Zp:Z", required=False,
                        help='If --write-precise-mapq is specified, store '
                             'original MAPQ in this extra SAM field')
    parser.add_argument('--keep-ztz', action='store_const',
                        const=True, default=False,
                        help='Don\'t remove ZT:Z field, with aligner-reported '
                             'feature data, from the final output SAM')

    # Prediction
    import model_fam
    model_fam.add_args(parser)
    parser.add_argument('--reweight-ratio', metavar='float', type=float,
                        default=1.0,
                        help='When fitting, reweigh samples so weight of '
                             'highest-mapq alignment has this times as '
                             'much weight as lowest-mapq.')
    parser.add_argument('--reweight-mapq', action='store_const', const=True,
                        default=False,
                        help='When fitting, reweigh samples according to '
                             'initially-predicted mapq.  Higher predictions '
                             'get more weight')
    parser.add_argument('--reweight-mapq-offset', metavar='float', type=float,
                        default=10.0,
                        help='Add this to every MAPQ before reweighting')
    parser.add_argument('--collapse', action='store_const', const=True,
                        default=False,
                        help='Remove redundant rows just before prediction. '
                             'Usually not a net win.')
    parser.add_argument('--max-rows', metavar='int', type=int, default=250000,
                        help='Maximum number of rows (alignments) to feed at '
                             'once to the prediction function')
    parser.add_argument('--no-oob', action='store_const', const=True,
                        default=False,
                        help='Don\'t use out-of-bag score when fitting '
                             'hyperparameters -- use cross validation '
                             'instead.  No effect for models that don\'t '
                             'calculate OOB score.')
    parser.add_argument('--skip-rewrite', action='store_const', const=True,
                        default=False,
                        help='Skip the final SAM rewriting step; other '
                             'results, including any fit and '
                             'prediction assessments requested, are still '
                             'written.')

    # Profiling
    parser.add_argument('--profile-memory', action='store_const', const=True,
                        default=False,
                        help='Use guppy/heapy to profile memory and '
                             'periodically print heap usage')

    # Parameters affecting the number of fits/predictions done per run
    parser.add_argument('--predict-for-training', action='store_const',
                        const=True, default=False,
                        help='Make predictions and produce associated '
                             'plots/output files for training (tandem) data')
    parser.add_argument('--try-include-mapq', action='store_const',
                        const=True, default=False,
                        help='Try both with and without including the '
                             'tool-predicted MAPQ as a feature; '
                             'default: does not include it')
    parser.add_argument('--subsampling-series', metavar='floats', type=str,
                        default='1.0',
                        help='Comma separated list of subsampling fractions '
                             'to try')
    parser.add_argument('--trials', metavar='int', type=int, default=1,
                        help='Number of times to repeat fitting/prediction')

    # Assessment of prediction accuracy
    parser.add_argument('--assess-accuracy', action='store_const', const=True,
                        default=False,
                        help='When correctness can be inferred from simulated '
                             'read names, assess accuracy of old and new MAPQ '
                             'predictions')
    parser.add_argument('--assess-limit', metavar='int', type=int,
                        default=100000000,
                        help='The maximum number of alignments to assess '
                             'when assessing accuracy')

    # Output file-related arguments
    parser.add_argument('--temp-directory', metavar='path', type=str,
                        required=False,
                        help='Write temporary files to this directory; when '
                             'None: uses environment variables '
                             'like TMPDIR, TEMP, etc')
    parser.add_argument('--output-directory', metavar='path', type=str,
                        help='Write outputs to this directory')
    parser.add_argument('--vanilla-output', metavar='path', type=str,
                        help='Only write final SAM file; suppress all other '
                             'output')
    parser.add_argument('--keep-intermediates', action='store_const',
                        const=True, default=False,
                        help='Keep intermediates in output directory; if '
                             'False, intermediates are written to a temporary '
                             'directory then deleted')


def go_profile(args, aligner_args, aligner_unpaired_args, aligner_paired_args):
    pr = None
    pstats = None
    if args['profile']:
        import cProfile
        import pstats
        pr = cProfile.Profile()
        pr.enable()
    go(args, aligner_args, aligner_unpaired_args, aligner_paired_args)
    if args['profile']:
        pr.disable()
        pstats.Stats(pr).sort_stats('tottime').print_stats(30)


def parse_aligner_parameters_from_argv(argv):
    argv = argv[:]
    sections = [[]]
    for arg in argv:
        if arg == '--':
            sections.append([])
        else:
            sections[-1].append(arg)
    new_argv = sections[0]
    aligner_args = [] if len(sections) < 2 else sections[1]
    aligner_unpaired_args = [] if len(sections) < 3 else sections[2]
    aligner_paired_args = [] if len(sections) < 4 else sections[3]
    return new_argv, aligner_args, aligner_unpaired_args, aligner_paired_args


if __name__ == "__main__":

    import argparse

    _parser = argparse.ArgumentParser(
        description='Align a collection of input reads, simulate a tandem '
                    'dataset, align the tandem dataset, and emit both the '
                    'input read alignments and the training data derived from '
                    'the tandem read alignments.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    if '--version' in sys.argv:
        print('Qtip' + open(join(bin_dir, 'VERSION')).read().rstrip())
        sys.exit(0)

    add_args(_parser)

    # Some basic flags
    _parser.add_argument('--profile', action='store_const', const=True,
                         default=False, help='Print profiling info')
    _parser.add_argument('--verbose', action='store_const', const=True,
                         default=False, help='Be talkative')
    _parser.add_argument('--version', action='store_const', const=True,
                         default=False, help='Print version and quit')

    _argv, _aligner_args, _aligner_unpaired_args, _aligner_paired_args = parse_aligner_parameters_from_argv(sys.argv)
    _args = _parser.parse_args(_argv[1:])

    go_profile(vars(_args), _aligner_args, _aligner_unpaired_args, _aligner_paired_args)
